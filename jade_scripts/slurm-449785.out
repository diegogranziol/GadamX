
	Python anaconda is now loaded in your environment.

numpy imported
Preparing directory ../out/ImageNet32/WideResNet28x10/Padam/seed=1_lr=0.03_wd=1e-05/
Using model WideResNet28x10
Loading ImageNet32 from /jmain01/home/JAD017/sjr02/dxg49-sjr02/curvature/data/
You are going to run models on the test set. Are you sure?
Using train (1281167) + test (50000)
Preparing model
{'depth': 28, 'widen_factor': 10}
Padam training
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
  ep        lr    tr_loss    tr_acc    te_loss    te_acc    te_top5_acc       time    mem_usage
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
   1    0.0300     4.7415   12.5870     4.0634   20.0800        43.0980  2218.3470       0.5569
   2    0.0300     3.3006   29.9656     3.2277   31.7460        57.5480  2220.6388       0.5569
   3    0.0300     2.7751   38.6980     2.8937   37.4640        63.8760  2218.4413       0.5569
   4    0.0300     2.4644   44.2218     2.6123   42.2340        68.4900  2217.1721       0.5569
   5    0.0300     2.2457   48.2843     2.4798   45.0460        70.8500  2299.5488       0.5569
   6    0.0300     2.0776   51.4173     2.3753   46.9880        72.7320  2301.7028       0.5569
   7    0.0300     1.9430   54.0291     2.2860   48.5500        73.9340  2351.2893       0.5569
   8    0.0300     1.8308   56.1881     2.2739   49.4240        74.9060  2367.9940       0.5569
   9    0.0300     1.7335   58.0829     2.2028   50.7640        75.6760  2325.5525       0.5569
  10    0.0300     1.6478   59.8132     2.1832   51.4940        76.2801  2318.2884       0.5569
  11    0.0300     1.5710   61.2990     2.1360   52.3640        77.0560  2317.2193       0.5569
  12    0.0300     1.5035   62.6637     2.1416   52.2420        76.9780  2295.7264       0.5569
  13    0.0300     1.4391   63.9739     2.1529   52.6480        77.2800  2303.8434       0.5569
  14    0.0300     1.3818   65.1283     2.1282   53.5660        77.6520  2330.4353       0.5569
  15    0.0300     1.3272   66.1744     2.1310   53.3780        77.4181  2326.7158       0.5569
  16    0.0300     1.2775   67.1975     2.1757   53.4580        77.5860  2343.9408       0.5569
  17    0.0300     1.2286   68.2321     2.1542   54.0640        77.9300  2335.0248       0.5569
  18    0.0300     1.1860   69.0990     2.1709   54.0880        77.9380  2358.0433       0.5569
  19    0.0300     1.1419   70.0495     2.2024   53.9700        77.9900  2333.1206       0.5569
  20    0.0300     1.1035   70.8155     2.1976   53.7160        77.8200  2347.6102       0.5569
  21    0.0300     1.0650   71.7085     2.2742   53.7300        77.5620  2321.7768       0.5569
  22    0.0300     1.0296   72.4015     2.2382   53.7960        77.8680  2369.0151       0.5569
  23    0.0300     0.9957   73.1323     2.2516   54.0740        78.1120  2350.2714       0.5569
  24    0.0300     0.9641   73.8301     2.2705   54.0240        77.6860  2320.3162       0.5569
  25    0.0300     0.9329   74.4756     2.3166   54.0200        77.8281  2319.0478       0.5569
  26    0.0300     0.9046   75.1050     2.3532   53.7560        77.5620  2351.8903       0.5569
  27    0.0285     0.8490   76.4363     2.3854   53.8540        77.6460  2381.3206       0.5569
  28    0.0270     0.7955   77.7460     2.4686   53.6860        77.3400  2319.3939       0.5569
  29    0.0255     0.7412   79.0528     2.4476   54.2440        77.9121  2309.9513       0.5569
  30    0.0241     0.6869   80.3919     2.4832   54.1060        77.7341  2301.8565       0.5569
  31    0.0226     0.6352   81.7154     2.5242   54.4600        77.8420  2318.0895       0.5569
  32    0.0211     0.5836   83.0553     2.5673   54.1820        77.7620  2306.5184       0.5569
  33    0.0196     0.5332   84.3921     2.5967   54.2540        77.9320  2300.6775       0.5569
  34    0.0181     0.4813   85.7552     2.6484   54.0480        77.7920  2297.6316       0.5569
  35    0.0166     0.4318   87.1583     2.6685   54.4680        78.1021  2262.3749       0.5569
  36    0.0152     0.3844   88.5527     2.8128   54.1380        77.6500  2340.2130       0.5569
  37    0.0137     0.3373   89.9185     2.7978   54.5260        77.8800  2298.2198       0.5569
  38    0.0122     0.2926   91.2876     2.8735   54.6580        77.8280  2299.4393       0.5569
  39    0.0107     0.2514   92.5761     2.8678   54.6920        78.0760  2312.1616       0.5569
  40    0.0092     0.2136   93.7822     2.9327   54.7780        77.9720  2329.6843       0.5569
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
  ep        lr    tr_loss    tr_acc    te_loss    te_acc    te_top5_acc       time    mem_usage
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
  41    0.0077     0.1804   94.8637     2.9699   54.8920        78.2840  2368.3701       0.5569
  42    0.0062     0.1522   95.7952     2.9979   55.1460        78.4201  2368.0615       0.5569
  43    0.0048     0.1283   96.6035     2.9853   55.1680        78.3640  2210.4518       0.5569
  44    0.0033     0.1095   97.2079     3.0786   55.0800        78.0961  2224.3504       0.5569
  45    0.0018     0.0950   97.6771     3.0080   55.5380        78.5400  2210.3855       0.5569
  46    0.0003     0.0863   97.9724     3.0445   55.4960        78.4461  2220.5078       0.5569
  47    0.0003     0.0841   98.0513     3.0699   55.6640        78.5260  2210.5745       0.5569
  48    0.0003     0.0827   98.0967     3.0534   55.4100        78.3861  2218.6074       0.5569
  49    0.0003     0.0819   98.1041     3.0660   55.7520        78.4940  2210.4783       0.5569
  50    0.0003     0.0812   98.1252     3.0855   55.4860        78.2140  2221.6040       0.5569
