
	Python anaconda is now loaded in your environment.

numpy imported
Preparing directory ../out/ImageNet32/WideResNet28x10/Padam/seed=1_lr=0.03_wd=1e-06/
Using model WideResNet28x10
Loading ImageNet32 from /jmain01/home/JAD017/sjr02/dxg49-sjr02/curvature/data/
You are going to run models on the test set. Are you sure?
Using train (1281167) + test (50000)
Preparing model
{'depth': 28, 'widen_factor': 10}
Padam training
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
  ep        lr    tr_loss    tr_acc    te_loss    te_acc    te_top5_acc       time    mem_usage
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
   1    0.0300     4.7413   12.6154     3.9654   20.9460        44.2160  2247.5599       0.5569
   2    0.0300     3.2962   30.0400     3.1973   32.0100        58.0320  2247.4939       0.5569
   3    0.0300     2.7682   38.8125     2.8846   37.9540        63.9180  2250.4744       0.5569
   4    0.0300     2.4564   44.3616     2.6233   42.3520        68.2540  2252.1270       0.5569
   5    0.0300     2.2362   48.4724     2.4668   45.0800        71.1460  2249.8556       0.5569
   6    0.0300     2.0676   51.6103     2.3806   47.1880        72.7100  2258.2460       0.5569
   7    0.0300     1.9306   54.2965     2.2866   48.7980        73.8960  2248.4380       0.5569
   8    0.0300     1.8140   56.5523     2.2407   49.7820        75.1060  2248.6006       0.5569
   9    0.0300     1.7140   58.5552     2.1987   50.6640        75.6500  2248.2059       0.5569
  10    0.0300     1.6252   60.2323     2.1819   51.3340        76.0820  2249.5817       0.5569
  11    0.0300     1.5445   61.8857     2.1446   52.4740        77.0940  2249.3798       0.5569
  12    0.0300     1.4727   63.3439     2.1342   52.3280        77.0720  2262.2012       0.5569
  13    0.0300     1.4055   64.6757     2.1479   52.7840        77.3500  2247.6992       0.5569
  14    0.0300     1.3421   65.9514     2.1304   53.6100        77.8880  2248.6639       0.5569
  15    0.0300     1.2816   67.2197     2.1558   53.4740        77.7420  2251.4342       0.5569
  16    0.0300     1.2272   68.3106     2.1815   53.4220        77.5760  2248.0342       0.5569
  17    0.0300     1.1739   69.4224     2.2042   53.7100        77.7741  2249.2944       0.5569
  18    0.0300     1.1234   70.4910     2.2262   53.9580        77.9880  2249.6687       0.5569
  19    0.0300     1.0765   71.5083     2.2261   54.1100        78.1580  2248.1680       0.5569
  20    0.0300     1.0320   72.4499     2.2341   53.8940        77.8700  2249.7433       0.5569
  21    0.0300     0.9886   73.3914     2.2987   53.6840        77.7460  2247.8905       0.5569
  22    0.0300     0.9467   74.2934     2.2893   54.2040        77.8860  2246.6188       0.5569
  23    0.0300     0.9083   75.1468     2.3138   53.9960        77.9180  2246.3982       0.5569
  24    0.0300     0.8720   75.9203     2.3513   54.0540        77.9860  2248.6981       0.5569
  25    0.0300     0.8335   76.8055     2.4581   53.5920        77.6040  2245.7561       0.5569
  26    0.0300     0.8014   77.4693     2.4457   53.4460        77.6500  2245.6770       0.5569
  27    0.0285     0.7435   78.9380     2.5319   53.4420        77.3400  2247.2729       0.5569
  28    0.0270     0.6882   80.3353     2.6202   53.1560        77.0980  2246.2973       0.5569
  29    0.0255     0.6339   81.6894     2.6055   53.8440        77.7820  2248.3862       0.5569
  30    0.0241     0.5797   83.0959     2.6761   53.7200        77.5240  2247.3405       0.5569
  31    0.0226     0.5299   84.3934     2.7174   53.9040        77.6380  2247.9171       0.5569
  32    0.0211     0.4818   85.6794     2.7672   53.8460        77.3721  2248.5229       0.5569
  33    0.0196     0.4334   87.0408     2.7975   53.8040        77.5540  2247.4615       0.5569
  34    0.0181     0.3895   88.2800     2.9101   53.8340        77.5640  2247.6701       0.5569
  35    0.0166     0.3463   89.4719     3.0006   53.8880        77.7100  2247.6456       0.5569
  36    0.0152     0.3070   90.7008     3.0285   53.8700        77.4660  2247.0708       0.5569
  37    0.0137     0.2686   91.8800     3.0674   54.1260        77.6780  2245.8979       0.5569
  38    0.0122     0.2331   92.9130     3.1900   54.1820        77.5561  2234.3934       0.5569
  39    0.0107     0.2028   93.9135     3.1506   54.3600        77.6400  2234.6741       0.5569
  40    0.0092     0.1740   94.8604     3.2313   54.2280        77.8040  2241.8947       0.5569
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
  ep        lr    tr_loss    tr_acc    te_loss    te_acc    te_top5_acc       time    mem_usage
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
  41    0.0077     0.1504   95.6378     3.2558   54.6880        77.9220  2243.9926       0.5569
  42    0.0062     0.1299   96.3114     3.3246   54.7200        77.9721  2249.0562       0.5569
  43    0.0048     0.1130   96.8643     3.3684   54.5620        77.9160  2244.6559       0.5569
  44    0.0033     0.0990   97.3391     3.4107   54.6880        77.9500  2249.3722       0.5569
  45    0.0018     0.0887   97.6870     3.3394   54.9440        78.1620  2246.3370       0.5569
  46    0.0003     0.0816   97.9363     3.4174   54.6900        77.9920  2244.4954       0.5569
  47    0.0003     0.0804   97.9722     3.3701   54.8019        78.2340  2245.3742       0.5569
  48    0.0003     0.0794   97.9989     3.4352   54.5560        77.9200  2246.9902       0.5569
  49    0.0003     0.0785   98.0273     3.3854   54.9700        78.1080  2249.7061       0.5569
  50    0.0003     0.0775   98.0603     3.3858   54.9660        78.2160  2248.6852       0.5569
