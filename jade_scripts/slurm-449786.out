
	Python anaconda is now loaded in your environment.

numpy imported
Preparing directory ../out/ImageNet32/WideResNet28x10/Padam/seed=1_lr=0.03_wd=1e-06/
Using model WideResNet28x10
Loading ImageNet32 from /jmain01/home/JAD017/sjr02/dxg49-sjr02/curvature/data/
You are going to run models on the test set. Are you sure?
Using train (1281167) + test (50000)
Preparing model
{'depth': 28, 'widen_factor': 10}
Padam training
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
  ep        lr    tr_loss    tr_acc    te_loss    te_acc    te_top5_acc       time    mem_usage
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
   1    0.0300     4.7458   12.5594     3.9227   21.3720        44.8280  2250.6487       0.5569
   2    0.0300     3.3018   29.9393     3.2509   31.4160        57.1080  2248.0275       0.5569
   3    0.0300     2.7732   38.7348     2.8707   37.8860        64.1060  2245.9591       0.5569
   4    0.0300     2.4599   44.3132     2.6283   42.1540        68.2020  2248.7779       0.5569
   5    0.0300     2.2387   48.3964     2.4807   45.1420        71.1200  2245.8876       0.5569
   6    0.0300     2.0687   51.6225     2.3752   47.1520        72.4920  2246.6555       0.5569
   7    0.0300     1.9313   54.2567     2.3244   47.7000        73.2340  2244.8607       0.5569
   8    0.0300     1.8153   56.5033     2.2826   49.4340        74.7360  2244.2946       0.5569
   9    0.0300     1.7154   58.5265     2.1963   50.8860        75.5240  2243.8463       0.5569
  10    0.0300     1.6264   60.2259     2.1735   51.5300        76.1500  2243.9257       0.5569
  11    0.0300     1.5454   61.8456     2.1748   51.8140        76.5660  2243.8036       0.5569
  12    0.0300     1.4735   63.3015     2.1427   52.3920        76.9320  2272.2325       0.5569
  13    0.0300     1.4049   64.6917     2.1525   52.8260        77.2680  2245.8716       0.5569
  14    0.0300     1.3425   65.9523     2.1375   53.5460        77.5380  2242.5140       0.5569
  15    0.0300     1.2827   67.1775     2.1427   53.8640        77.7340  2242.4331       0.5569
  16    0.0300     1.2271   68.3529     2.1991   53.3080        77.4700  2245.4456       0.5569
  17    0.0300     1.1752   69.3910     2.1776   53.8300        77.7861  2242.9057       0.5569
  18    0.0300     1.1235   70.4897     2.1956   53.9300        78.1780  2246.9922       0.5569
  19    0.0300     1.0774   71.4780     2.2409   53.9400        77.8801  2241.7425       0.5569
  20    0.0300     1.0307   72.4804     2.2993   53.1540        77.2860  2242.6288       0.5569
  21    0.0300     0.9893   73.3629     2.3179   53.7100        77.7400  2243.1079       0.5569
  22    0.0300     0.9473   74.2843     2.3598   53.3020        77.2040  2236.2933       0.5569
  23    0.0300     0.9074   75.1595     2.3457   53.7640        77.8080  2235.1719       0.5569
  24    0.0300     0.8698   76.0262     2.3268   53.9900        77.9780  2239.6459       0.5569
  25    0.0300     0.8337   76.7781     2.4582   53.6080        77.5080  2235.8421       0.5569
  26    0.0300     0.7999   77.5587     2.4297   53.8060        77.7180  2235.5838       0.5569
  27    0.0285     0.7438   78.9359     2.4895   53.5260        77.4440  2234.7382       0.5569
  28    0.0270     0.6880   80.3643     2.5760   53.6060        77.5201  2234.6731       0.5569
  29    0.0255     0.6335   81.7243     2.6260   53.6980        77.4921  2233.9392       0.5569
  30    0.0241     0.5802   83.0975     2.6845   53.6920        77.4021  2232.6480       0.5569
  31    0.0226     0.5284   84.4347     2.7582   53.2280        77.2121  2234.5275       0.5569
  32    0.0211     0.4817   85.7144     2.8099   53.4760        77.1900  2232.0656       0.5569
  33    0.0196     0.4346   86.9930     2.8063   53.7580        77.5660  2231.6691       0.5569
  34    0.0181     0.3896   88.2669     2.9233   53.6740        77.2080  2234.6021       0.5569
  35    0.0166     0.3454   89.5512     2.9919   53.8540        77.2861  2232.3756       0.5569
  36    0.0152     0.3064   90.7331     3.0688   53.9660        77.3820  2231.9032       0.5569
  37    0.0137     0.2682   91.8605     3.1125   54.0020        77.4941  2238.1537       0.5569
  38    0.0122     0.2337   92.9164     3.1946   53.7660        77.3160  2233.5830       0.5569
  39    0.0107     0.2024   93.9147     3.1672   54.0620        77.5740  2231.0227       0.5569
  40    0.0092     0.1745   94.8493     3.2504   54.0300        77.6500  2240.5433       0.5569
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
  ep        lr    tr_loss    tr_acc    te_loss    te_acc    te_top5_acc       time    mem_usage
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
  41    0.0077     0.1507   95.6071     3.2849   54.1300        77.6281  2243.7687       0.5569
  42    0.0062     0.1297   96.3148     3.3424   54.2540        77.7701  2250.8229       0.5569
  43    0.0048     0.1134   96.8516     3.3312   54.4680        77.9340  2244.0252       0.5569
  44    0.0033     0.0993   97.3196     3.4262   54.3500        77.7460  2250.4891       0.5569
  45    0.0018     0.0887   97.6681     3.3675   54.6080        77.9900  2247.2715       0.5569
  46    0.0003     0.0815   97.9348     3.4029   54.5920        77.8840  2247.2815       0.5569
  47    0.0003     0.0800   97.9758     3.4051   54.5140        77.8860  2246.1423       0.5569
  48    0.0003     0.0792   98.0079     3.3842   54.6600        77.8340  2247.4479       0.5569
  49    0.0003     0.0783   98.0188     3.4005   54.5700        77.9200  2249.0208       0.5569
  50    0.0003     0.0775   98.0456     3.4292   54.3620        77.8100  2248.8376       0.5569
