
	Python anaconda is now loaded in your environment.

numpy imported
Preparing directory ../out/ImageNet32/WideResNet28x10/SGD/seed=1_lr=0.03_mom=0.9_wd=8e-06/
Using model WideResNet28x10
Loading ImageNet32 from /jmain01/home/JAD017/sjr02/dxg49-sjr02/curvature/data/
You are going to run models on the test set. Are you sure?
Using train (1281167) + test (50000)
Preparing model
{'depth': 28, 'widen_factor': 10}
SGD training
Linearly decaying learning rate schedule
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
  ep        lr    tr_loss    tr_acc    te_loss    te_acc    te_top5_acc       time    mem_usage
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
   1    0.0300     4.6471   13.5777     3.7072   23.9380        48.1320  2926.5640       0.4175
   2    0.0300     3.2167   31.4015     3.0442   34.3840        60.5040  2936.2950       0.4175
   3    0.0300     2.6924   40.2321     2.6705   41.0060        67.1580  2930.7026       0.4175
   4    0.0300     2.3919   45.6238     2.4987   44.1440        69.9660  2930.7362       0.4175
   5    0.0300     2.1850   49.5078     2.3696   46.7660        72.1720  2929.1416       0.4175
   6    0.0300     2.0286   52.4604     2.2672   48.3920        74.1340  2926.6514       0.4175
   7    0.0300     1.9037   54.8917     2.2208   49.7940        74.8080  2929.0068       0.4175
   8    0.0300     1.8014   56.8445     2.1613   51.1940        75.8120  2927.6346       0.4175
   9    0.0300     1.7120   58.6546     2.1242   51.9540        76.7280  2928.1724       0.4175
  10    0.0300     1.6353   60.0776     2.1072   52.3540        76.8880  2927.6080       0.4175
  11    0.0300     1.5664   61.4500     2.0673   53.2780        77.7020  2929.4507       0.4175
  12    0.0300     1.5035   62.7355     2.0665   53.5520        77.7680  2926.1132       0.4175
  13    0.0300     1.4459   63.9235     2.0556   54.2060        78.0221  2925.3101       0.4175
  14    0.0300     1.3943   64.8742     2.0743   54.0080        78.1040  2929.2345       0.4175
  15    0.0300     1.3446   65.9226     2.0640   54.1740        78.4040  2924.2893       0.4175
  16    0.0300     1.3000   66.8063     2.0733   54.4100        78.6820  2925.7285       0.4175
  17    0.0300     1.2560   67.6782     2.0759   54.4540        78.3500  2926.5828       0.4175
  18    0.0300     1.2161   68.5334     2.0826   54.8720        78.5380  2925.0833       0.4175
  19    0.0300     1.1792   69.3095     2.0761   54.9680        78.9260  2925.1477       0.4175
  20    0.0300     1.1423   70.0676     2.1146   54.6480        78.6801  2929.3965       0.4175
  21    0.0300     1.1085   70.7549     2.1218   54.9020        78.6740  2926.5950       0.4175
  22    0.0300     1.0755   71.4213     2.1338   54.7320        78.5300  2931.2130       0.4175
  23    0.0300     1.0442   72.1106     2.1546   54.6080        78.4760  2924.9296       0.4175
  24    0.0300     1.0156   72.7130     2.1731   54.5960        78.4180  2924.2507       0.4175
  25    0.0300     0.9875   73.2658     2.2016   54.5540        78.5380  2926.7539       0.4175
  26    0.0300     0.9601   73.9173     2.1807   54.8080        78.5021  2925.2463       0.4175
  27    0.0285     0.9076   75.1754     2.2221   54.9280        78.3321  2925.8492       0.4175
  28    0.0270     0.8541   76.3790     2.2775   54.5740        78.2820  2924.7538       0.4175
  29    0.0255     0.8005   77.6459     2.2916   54.9880        78.3920  2924.7812       0.4175
  30    0.0241     0.7471   78.9309     2.3198   54.8140        78.4401  2926.4935       0.4175
  31    0.0226     0.6964   80.1817     2.3446   55.0320        78.7860  2925.7949       0.4175
  32    0.0211     0.6428   81.5589     2.4304   54.5720        78.0940  2925.1439       0.4175
  33    0.0196     0.5901   82.8924     2.4456   54.9800        78.4321  2926.2872       0.4175
  34    0.0181     0.5369   84.2487     2.5121   54.8560        78.4320  2925.6878       0.4175
  35    0.0166     0.4819   85.7820     2.5526   55.0740        78.5700  2926.1934       0.4175
  36    0.0152     0.4301   87.1884     2.6184   55.0699        78.5680  2926.9238       0.4175
  37    0.0137     0.3749   88.7461     2.6571   55.2400        78.5800  2928.6878       0.4175
  38    0.0122     0.3243   90.2281     2.7269   55.1360        78.3920  2922.5743       0.4175
  39    0.0107     0.2727   91.8540     2.7959   55.1260        78.6960  2925.0361       0.4175
  40    0.0092     0.2277   93.2174     2.8203   55.7860        78.8020  2924.4009       0.4175
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
  ep        lr    tr_loss    tr_acc    te_loss    te_acc    te_top5_acc       time    mem_usage
----  --------  ---------  --------  ---------  --------  -------------  ---------  -----------
  41    0.0077     0.1867   94.5471     2.8768   55.6520        78.8180  2924.5485       0.4175
  42    0.0062     0.1522   95.6762     2.9224   55.8200        78.9300  2924.5532       0.4175
  43    0.0048     0.1236   96.6212     2.9481   55.7680        78.9620  2923.2382       0.4175
  44    0.0033     0.1031   97.2965     2.9554   56.0180        79.0620  2923.3734       0.4175
  45    0.0018     0.0876   97.7969     2.9624   56.2080        79.0921  2925.7969       0.4175
  46    0.0003     0.0780   98.0892     2.9635   56.2580        79.1960  2923.9427       0.4175
  47    0.0003     0.0766   98.1574     2.9632   56.2640        79.2460  2927.8273       0.4175
  48    0.0003     0.0750   98.1996     2.9690   56.2600        79.2160  2929.7060       0.4175
  49    0.0003     0.0746   98.2016     2.9712   56.2280        79.2160  2925.0431       0.4175
  50    0.0003     0.0733   98.2592     2.9737   56.2380        79.2020  2926.4895       0.4175
